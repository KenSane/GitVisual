{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\janry\\\\Music\\\\VisualQuery\\\\frontend\\\\src\\\\component\\\\EmotionDetection.js\",\n  _s = $RefreshSig$();\nimport React, { useRef, useEffect, useState } from 'react';\nimport * as faceapi from 'face-api.js';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst EmotionDetection = _ref => {\n  _s();\n  let {\n    setEmotion\n  } = _ref;\n  const videoRef = useRef(null);\n  useEffect(() => {\n    const loadModels = async () => {\n      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');\n      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');\n      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\n      await faceapi.nets.faceExpressionNet.loadFromUri('/models');\n    };\n    const startVideo = () => {\n      navigator.mediaDevices.getUserMedia({\n        video: {}\n      }).then(stream => {\n        if (videoRef.current) {\n          videoRef.current.srcObject = stream;\n        }\n      }).catch(err => console.error('Error accessing webcam: ', err));\n    };\n    const detectEmotion = async () => {\n      if (videoRef.current) {\n        const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n        if (detections.length > 0) {\n          const emotions = detections[0].expressions;\n          const maxValue = Math.max(...Object.values(emotions));\n          const emotion = Object.keys(emotions).find(key => emotions[key] === maxValue);\n          setEmotion(emotion);\n        }\n      }\n    };\n    loadModels().then(startVideo);\n    const intervalId = setInterval(detectEmotion, 1000); // Check for emotions every second\n\n    return () => {\n      clearInterval(intervalId);\n      if (videoRef.current && videoRef.current.srcObject) {\n        videoRef.current.srcObject.getTracks().forEach(track => track.stop());\n      }\n    };\n  }, [setEmotion]);\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: /*#__PURE__*/_jsxDEV(\"video\", {\n      ref: videoRef,\n      autoPlay: true,\n      muted: true,\n      width: \"640\",\n      height: \"480\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 51,\n      columnNumber: 7\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 50,\n    columnNumber: 5\n  }, this);\n};\n_s(EmotionDetection, \"PdMsmLAy5JKU3vCrhAlqGYQfKuA=\");\n_c = EmotionDetection;\nexport default EmotionDetection;\nvar _c;\n$RefreshReg$(_c, \"EmotionDetection\");","map":{"version":3,"names":["React","useRef","useEffect","useState","faceapi","jsxDEV","_jsxDEV","EmotionDetection","_ref","_s","setEmotion","videoRef","loadModels","nets","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","startVideo","navigator","mediaDevices","getUserMedia","video","then","stream","current","srcObject","catch","err","console","error","detectEmotion","detections","detectAllFaces","TinyFaceDetectorOptions","withFaceLandmarks","withFaceExpressions","length","emotions","expressions","maxValue","Math","max","Object","values","emotion","keys","find","key","intervalId","setInterval","clearInterval","getTracks","forEach","track","stop","children","ref","autoPlay","muted","width","height","fileName","_jsxFileName","lineNumber","columnNumber","_c","$RefreshReg$"],"sources":["C:/Users/janry/Music/VisualQuery/frontend/src/component/EmotionDetection.js"],"sourcesContent":["import React, { useRef, useEffect, useState } from 'react';\r\nimport * as faceapi from 'face-api.js';\r\n\r\nconst EmotionDetection = ({ setEmotion }) => {\r\n  const videoRef = useRef(null);\r\n\r\n  useEffect(() => {\r\n    const loadModels = async () => {\r\n      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');\r\n      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');\r\n      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\r\n      await faceapi.nets.faceExpressionNet.loadFromUri('/models');\r\n    };\r\n\r\n    const startVideo = () => {\r\n      navigator.mediaDevices.getUserMedia({ video: {} })\r\n        .then(stream => {\r\n          if (videoRef.current) {\r\n            videoRef.current.srcObject = stream;\r\n          }\r\n        })\r\n        .catch(err => console.error('Error accessing webcam: ', err));\r\n    };\r\n\r\n    const detectEmotion = async () => {\r\n      if (videoRef.current) {\r\n        const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\r\n        if (detections.length > 0) {\r\n          const emotions = detections[0].expressions;\r\n          const maxValue = Math.max(...Object.values(emotions));\r\n          const emotion = Object.keys(emotions).find(key => emotions[key] === maxValue);\r\n          setEmotion(emotion);\r\n        }\r\n      }\r\n    };\r\n\r\n    loadModels().then(startVideo);\r\n\r\n    const intervalId = setInterval(detectEmotion, 1000); // Check for emotions every second\r\n\r\n    return () => {\r\n      clearInterval(intervalId);\r\n      if (videoRef.current && videoRef.current.srcObject) {\r\n        videoRef.current.srcObject.getTracks().forEach(track => track.stop());\r\n      }\r\n    };\r\n  }, [setEmotion]);\r\n\r\n  return (\r\n    <div>\r\n      <video ref={videoRef} autoPlay muted width=\"640\" height=\"480\" />\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default EmotionDetection;\r\n"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,MAAM,EAAEC,SAAS,EAAEC,QAAQ,QAAQ,OAAO;AAC1D,OAAO,KAAKC,OAAO,MAAM,aAAa;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAEvC,MAAMC,gBAAgB,GAAGC,IAAA,IAAoB;EAAAC,EAAA;EAAA,IAAnB;IAAEC;EAAW,CAAC,GAAAF,IAAA;EACtC,MAAMG,QAAQ,GAAGV,MAAM,CAAC,IAAI,CAAC;EAE7BC,SAAS,CAAC,MAAM;IACd,MAAMU,UAAU,GAAG,MAAAA,CAAA,KAAY;MAC7B,MAAMR,OAAO,CAACS,IAAI,CAACC,gBAAgB,CAACC,WAAW,CAAC,SAAS,CAAC;MAC1D,MAAMX,OAAO,CAACS,IAAI,CAACG,iBAAiB,CAACD,WAAW,CAAC,SAAS,CAAC;MAC3D,MAAMX,OAAO,CAACS,IAAI,CAACI,kBAAkB,CAACF,WAAW,CAAC,SAAS,CAAC;MAC5D,MAAMX,OAAO,CAACS,IAAI,CAACK,iBAAiB,CAACH,WAAW,CAAC,SAAS,CAAC;IAC7D,CAAC;IAED,MAAMI,UAAU,GAAGA,CAAA,KAAM;MACvBC,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;QAAEC,KAAK,EAAE,CAAC;MAAE,CAAC,CAAC,CAC/CC,IAAI,CAACC,MAAM,IAAI;QACd,IAAId,QAAQ,CAACe,OAAO,EAAE;UACpBf,QAAQ,CAACe,OAAO,CAACC,SAAS,GAAGF,MAAM;QACrC;MACF,CAAC,CAAC,CACDG,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,0BAA0B,EAAEF,GAAG,CAAC,CAAC;IACjE,CAAC;IAED,MAAMG,aAAa,GAAG,MAAAA,CAAA,KAAY;MAChC,IAAIrB,QAAQ,CAACe,OAAO,EAAE;QACpB,MAAMO,UAAU,GAAG,MAAM7B,OAAO,CAAC8B,cAAc,CAACvB,QAAQ,CAACe,OAAO,EAAE,IAAItB,OAAO,CAAC+B,uBAAuB,CAAC,CAAC,CAAC,CAACC,iBAAiB,CAAC,CAAC,CAACC,mBAAmB,CAAC,CAAC;QAClJ,IAAIJ,UAAU,CAACK,MAAM,GAAG,CAAC,EAAE;UACzB,MAAMC,QAAQ,GAAGN,UAAU,CAAC,CAAC,CAAC,CAACO,WAAW;UAC1C,MAAMC,QAAQ,GAAGC,IAAI,CAACC,GAAG,CAAC,GAAGC,MAAM,CAACC,MAAM,CAACN,QAAQ,CAAC,CAAC;UACrD,MAAMO,OAAO,GAAGF,MAAM,CAACG,IAAI,CAACR,QAAQ,CAAC,CAACS,IAAI,CAACC,GAAG,IAAIV,QAAQ,CAACU,GAAG,CAAC,KAAKR,QAAQ,CAAC;UAC7E/B,UAAU,CAACoC,OAAO,CAAC;QACrB;MACF;IACF,CAAC;IAEDlC,UAAU,CAAC,CAAC,CAACY,IAAI,CAACL,UAAU,CAAC;IAE7B,MAAM+B,UAAU,GAAGC,WAAW,CAACnB,aAAa,EAAE,IAAI,CAAC,CAAC,CAAC;;IAErD,OAAO,MAAM;MACXoB,aAAa,CAACF,UAAU,CAAC;MACzB,IAAIvC,QAAQ,CAACe,OAAO,IAAIf,QAAQ,CAACe,OAAO,CAACC,SAAS,EAAE;QAClDhB,QAAQ,CAACe,OAAO,CAACC,SAAS,CAAC0B,SAAS,CAAC,CAAC,CAACC,OAAO,CAACC,KAAK,IAAIA,KAAK,CAACC,IAAI,CAAC,CAAC,CAAC;MACvE;IACF,CAAC;EACH,CAAC,EAAE,CAAC9C,UAAU,CAAC,CAAC;EAEhB,oBACEJ,OAAA;IAAAmD,QAAA,eACEnD,OAAA;MAAOoD,GAAG,EAAE/C,QAAS;MAACgD,QAAQ;MAACC,KAAK;MAACC,KAAK,EAAC,KAAK;MAACC,MAAM,EAAC;IAAK;MAAAC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAE;EAAC;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OAC7D,CAAC;AAEV,CAAC;AAACzD,EAAA,CAlDIF,gBAAgB;AAAA4D,EAAA,GAAhB5D,gBAAgB;AAoDtB,eAAeA,gBAAgB;AAAC,IAAA4D,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}