{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\janry\\\\Music\\\\VisualQuery\\\\frontend\\\\src\\\\component\\\\HeadPoseAndEmotion.js\",\n  _s = $RefreshSig$();\nimport React, { useRef, useEffect, useState } from 'react';\nimport * as mp from '@mediapipe/face_mesh';\nimport * as cam from '@mediapipe/camera_utils';\nimport * as faceapi from 'face-api.js';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst HeadPoseAndEmotionDetection = _ref => {\n  _s();\n  let {\n    setHeadPose,\n    setEmotion\n  } = _ref;\n  const videoRef = useRef(null);\n  useEffect(() => {\n    const faceMesh = new mp.FaceMesh({\n      locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n    });\n    faceMesh.setOptions({\n      maxNumFaces: 1,\n      refineLandmarks: true,\n      minDetectionConfidence: 0.5,\n      minTrackingConfidence: 0.5\n    });\n    faceMesh.onResults(onResults);\n    let camera;\n    if (typeof videoRef.current !== 'undefined' && videoRef.current !== null) {\n      camera = new cam.Camera(videoRef.current, {\n        onFrame: async () => {\n          await faceMesh.send({\n            image: videoRef.current\n          });\n        },\n        width: 640,\n        height: 480\n      });\n      camera.start();\n    }\n    const loadFaceApiModels = async () => {\n      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');\n      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');\n      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\n      await faceapi.nets.faceExpressionNet.loadFromUri('/models');\n    };\n    const detectEmotion = async () => {\n      if (videoRef.current) {\n        const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n        if (detections.length > 0) {\n          const emotions = detections[0].expressions;\n          const maxValue = Math.max(...Object.values(emotions));\n          const emotion = Object.keys(emotions).find(key => emotions[key] === maxValue);\n          setEmotion(emotion);\n        }\n      }\n    };\n    loadFaceApiModels();\n    const intervalId = setInterval(detectEmotion, 1000); // Check for emotions every second\n\n    function onResults(results) {\n      if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {\n        const landmarks = results.multiFaceLandmarks[0];\n        const noseTip = landmarks[1];\n        const leftCheek = landmarks[33];\n        const rightCheek = landmarks[263];\n        const dx = rightCheek.x - leftCheek.x;\n        const dy = rightCheek.y - leftCheek.y;\n        const angleY = Math.atan2(dy, dx) * (180 / Math.PI);\n        const noseBottom = landmarks[2];\n        const chin = landmarks[152];\n        const dz = chin.y - noseBottom.y;\n        const angleX = Math.atan2(dz, dx) * (180 / Math.PI);\n        let pose = 'forward';\n        if (angleY < -10) {\n          pose = 'left';\n        } else if (angleX > 17) {\n          pose = 'up';\n        } else if (angleX < -1) {\n          pose = 'down';\n        }\n        setHeadPose(pose);\n      }\n    }\n    return () => {\n      clearInterval(intervalId);\n      if (camera) {\n        camera.stop();\n      }\n    };\n  }, [setHeadPose, setEmotion]);\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: /*#__PURE__*/_jsxDEV(\"video\", {\n      ref: videoRef,\n      autoPlay: true,\n      muted: true,\n      width: \"640\",\n      height: \"480\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 99,\n      columnNumber: 7\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 98,\n    columnNumber: 5\n  }, this);\n};\n_s(HeadPoseAndEmotionDetection, \"PdMsmLAy5JKU3vCrhAlqGYQfKuA=\");\n_c = HeadPoseAndEmotionDetection;\nexport default HeadPoseAndEmotionDetection;\nvar _c;\n$RefreshReg$(_c, \"HeadPoseAndEmotionDetection\");","map":{"version":3,"names":["React","useRef","useEffect","useState","mp","cam","faceapi","jsxDEV","_jsxDEV","HeadPoseAndEmotionDetection","_ref","_s","setHeadPose","setEmotion","videoRef","faceMesh","FaceMesh","locateFile","file","setOptions","maxNumFaces","refineLandmarks","minDetectionConfidence","minTrackingConfidence","onResults","camera","current","Camera","onFrame","send","image","width","height","start","loadFaceApiModels","nets","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","detectEmotion","detections","detectAllFaces","TinyFaceDetectorOptions","withFaceLandmarks","withFaceExpressions","length","emotions","expressions","maxValue","Math","max","Object","values","emotion","keys","find","key","intervalId","setInterval","results","multiFaceLandmarks","landmarks","noseTip","leftCheek","rightCheek","dx","x","dy","y","angleY","atan2","PI","noseBottom","chin","dz","angleX","pose","clearInterval","stop","children","ref","autoPlay","muted","fileName","_jsxFileName","lineNumber","columnNumber","_c","$RefreshReg$"],"sources":["C:/Users/janry/Music/VisualQuery/frontend/src/component/HeadPoseAndEmotion.js"],"sourcesContent":["import React, { useRef, useEffect, useState } from 'react';\r\nimport * as mp from '@mediapipe/face_mesh';\r\nimport * as cam from '@mediapipe/camera_utils';\r\nimport * as faceapi from 'face-api.js';\r\n\r\nconst HeadPoseAndEmotionDetection = ({ setHeadPose, setEmotion }) => {\r\n  const videoRef = useRef(null);\r\n\r\n  useEffect(() => {\r\n    const faceMesh = new mp.FaceMesh({\r\n      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,\r\n    });\r\n\r\n    faceMesh.setOptions({\r\n      maxNumFaces: 1,\r\n      refineLandmarks: true,\r\n      minDetectionConfidence: 0.5,\r\n      minTrackingConfidence: 0.5,\r\n    });\r\n\r\n    faceMesh.onResults(onResults);\r\n\r\n    let camera;\r\n    if (typeof videoRef.current !== 'undefined' && videoRef.current !== null) {\r\n      camera = new cam.Camera(videoRef.current, {\r\n        onFrame: async () => {\r\n          await faceMesh.send({ image: videoRef.current });\r\n        },\r\n        width: 640,\r\n        height: 480,\r\n      });\r\n      camera.start();\r\n    }\r\n\r\n    const loadFaceApiModels = async () => {\r\n      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');\r\n      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');\r\n      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\r\n      await faceapi.nets.faceExpressionNet.loadFromUri('/models');\r\n    };\r\n\r\n    const detectEmotion = async () => {\r\n      if (videoRef.current) {\r\n        const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\r\n        if (detections.length > 0) {\r\n          const emotions = detections[0].expressions;\r\n          const maxValue = Math.max(...Object.values(emotions));\r\n          const emotion = Object.keys(emotions).find(key => emotions[key] === maxValue);\r\n          setEmotion(emotion);\r\n        }\r\n      }\r\n    };\r\n\r\n    loadFaceApiModels();\r\n\r\n    const intervalId = setInterval(detectEmotion, 1000); // Check for emotions every second\r\n\r\n    function onResults(results) {\r\n      if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {\r\n        const landmarks = results.multiFaceLandmarks[0];\r\n\r\n        const noseTip = landmarks[1];\r\n        const leftCheek = landmarks[33];\r\n        const rightCheek = landmarks[263];\r\n\r\n        const dx = rightCheek.x - leftCheek.x;\r\n        const dy = rightCheek.y - leftCheek.y;\r\n        const angleY = Math.atan2(dy, dx) * (180 / Math.PI);\r\n\r\n        const noseBottom = landmarks[2];\r\n        const chin = landmarks[152];\r\n\r\n        const dz = chin.y - noseBottom.y;\r\n        const angleX = Math.atan2(dz, dx) * (180 / Math.PI);\r\n\r\n        let pose = 'forward';\r\n        if (angleY < -10) {\r\n          pose = 'left';\r\n        } else if (angleX > 17) {\r\n          pose = 'up';\r\n        } else if (angleX < -1) {\r\n          pose = 'down';\r\n        }\r\n\r\n        setHeadPose(pose);\r\n      }\r\n    }\r\n\r\n    return () => {\r\n      clearInterval(intervalId);\r\n      if (camera) {\r\n        camera.stop();\r\n      }\r\n    };\r\n  }, [setHeadPose, setEmotion]);\r\n\r\n  return (\r\n    <div>\r\n      <video ref={videoRef} autoPlay muted width=\"640\" height=\"480\" />\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default HeadPoseAndEmotionDetection;\r\n"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,MAAM,EAAEC,SAAS,EAAEC,QAAQ,QAAQ,OAAO;AAC1D,OAAO,KAAKC,EAAE,MAAM,sBAAsB;AAC1C,OAAO,KAAKC,GAAG,MAAM,yBAAyB;AAC9C,OAAO,KAAKC,OAAO,MAAM,aAAa;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAEvC,MAAMC,2BAA2B,GAAGC,IAAA,IAAiC;EAAAC,EAAA;EAAA,IAAhC;IAAEC,WAAW;IAAEC;EAAW,CAAC,GAAAH,IAAA;EAC9D,MAAMI,QAAQ,GAAGb,MAAM,CAAC,IAAI,CAAC;EAE7BC,SAAS,CAAC,MAAM;IACd,MAAMa,QAAQ,GAAG,IAAIX,EAAE,CAACY,QAAQ,CAAC;MAC/BC,UAAU,EAAGC,IAAI,IAAM,qDAAoDA,IAAK;IAClF,CAAC,CAAC;IAEFH,QAAQ,CAACI,UAAU,CAAC;MAClBC,WAAW,EAAE,CAAC;MACdC,eAAe,EAAE,IAAI;MACrBC,sBAAsB,EAAE,GAAG;MAC3BC,qBAAqB,EAAE;IACzB,CAAC,CAAC;IAEFR,QAAQ,CAACS,SAAS,CAACA,SAAS,CAAC;IAE7B,IAAIC,MAAM;IACV,IAAI,OAAOX,QAAQ,CAACY,OAAO,KAAK,WAAW,IAAIZ,QAAQ,CAACY,OAAO,KAAK,IAAI,EAAE;MACxED,MAAM,GAAG,IAAIpB,GAAG,CAACsB,MAAM,CAACb,QAAQ,CAACY,OAAO,EAAE;QACxCE,OAAO,EAAE,MAAAA,CAAA,KAAY;UACnB,MAAMb,QAAQ,CAACc,IAAI,CAAC;YAAEC,KAAK,EAAEhB,QAAQ,CAACY;UAAQ,CAAC,CAAC;QAClD,CAAC;QACDK,KAAK,EAAE,GAAG;QACVC,MAAM,EAAE;MACV,CAAC,CAAC;MACFP,MAAM,CAACQ,KAAK,CAAC,CAAC;IAChB;IAEA,MAAMC,iBAAiB,GAAG,MAAAA,CAAA,KAAY;MACpC,MAAM5B,OAAO,CAAC6B,IAAI,CAACC,gBAAgB,CAACC,WAAW,CAAC,SAAS,CAAC;MAC1D,MAAM/B,OAAO,CAAC6B,IAAI,CAACG,iBAAiB,CAACD,WAAW,CAAC,SAAS,CAAC;MAC3D,MAAM/B,OAAO,CAAC6B,IAAI,CAACI,kBAAkB,CAACF,WAAW,CAAC,SAAS,CAAC;MAC5D,MAAM/B,OAAO,CAAC6B,IAAI,CAACK,iBAAiB,CAACH,WAAW,CAAC,SAAS,CAAC;IAC7D,CAAC;IAED,MAAMI,aAAa,GAAG,MAAAA,CAAA,KAAY;MAChC,IAAI3B,QAAQ,CAACY,OAAO,EAAE;QACpB,MAAMgB,UAAU,GAAG,MAAMpC,OAAO,CAACqC,cAAc,CAAC7B,QAAQ,CAACY,OAAO,EAAE,IAAIpB,OAAO,CAACsC,uBAAuB,CAAC,CAAC,CAAC,CAACC,iBAAiB,CAAC,CAAC,CAACC,mBAAmB,CAAC,CAAC;QAClJ,IAAIJ,UAAU,CAACK,MAAM,GAAG,CAAC,EAAE;UACzB,MAAMC,QAAQ,GAAGN,UAAU,CAAC,CAAC,CAAC,CAACO,WAAW;UAC1C,MAAMC,QAAQ,GAAGC,IAAI,CAACC,GAAG,CAAC,GAAGC,MAAM,CAACC,MAAM,CAACN,QAAQ,CAAC,CAAC;UACrD,MAAMO,OAAO,GAAGF,MAAM,CAACG,IAAI,CAACR,QAAQ,CAAC,CAACS,IAAI,CAACC,GAAG,IAAIV,QAAQ,CAACU,GAAG,CAAC,KAAKR,QAAQ,CAAC;UAC7ErC,UAAU,CAAC0C,OAAO,CAAC;QACrB;MACF;IACF,CAAC;IAEDrB,iBAAiB,CAAC,CAAC;IAEnB,MAAMyB,UAAU,GAAGC,WAAW,CAACnB,aAAa,EAAE,IAAI,CAAC,CAAC,CAAC;;IAErD,SAASjB,SAASA,CAACqC,OAAO,EAAE;MAC1B,IAAIA,OAAO,CAACC,kBAAkB,IAAID,OAAO,CAACC,kBAAkB,CAACf,MAAM,GAAG,CAAC,EAAE;QACvE,MAAMgB,SAAS,GAAGF,OAAO,CAACC,kBAAkB,CAAC,CAAC,CAAC;QAE/C,MAAME,OAAO,GAAGD,SAAS,CAAC,CAAC,CAAC;QAC5B,MAAME,SAAS,GAAGF,SAAS,CAAC,EAAE,CAAC;QAC/B,MAAMG,UAAU,GAAGH,SAAS,CAAC,GAAG,CAAC;QAEjC,MAAMI,EAAE,GAAGD,UAAU,CAACE,CAAC,GAAGH,SAAS,CAACG,CAAC;QACrC,MAAMC,EAAE,GAAGH,UAAU,CAACI,CAAC,GAAGL,SAAS,CAACK,CAAC;QACrC,MAAMC,MAAM,GAAGpB,IAAI,CAACqB,KAAK,CAACH,EAAE,EAAEF,EAAE,CAAC,IAAI,GAAG,GAAGhB,IAAI,CAACsB,EAAE,CAAC;QAEnD,MAAMC,UAAU,GAAGX,SAAS,CAAC,CAAC,CAAC;QAC/B,MAAMY,IAAI,GAAGZ,SAAS,CAAC,GAAG,CAAC;QAE3B,MAAMa,EAAE,GAAGD,IAAI,CAACL,CAAC,GAAGI,UAAU,CAACJ,CAAC;QAChC,MAAMO,MAAM,GAAG1B,IAAI,CAACqB,KAAK,CAACI,EAAE,EAAET,EAAE,CAAC,IAAI,GAAG,GAAGhB,IAAI,CAACsB,EAAE,CAAC;QAEnD,IAAIK,IAAI,GAAG,SAAS;QACpB,IAAIP,MAAM,GAAG,CAAC,EAAE,EAAE;UAChBO,IAAI,GAAG,MAAM;QACf,CAAC,MAAM,IAAID,MAAM,GAAG,EAAE,EAAE;UACtBC,IAAI,GAAG,IAAI;QACb,CAAC,MAAM,IAAID,MAAM,GAAG,CAAC,CAAC,EAAE;UACtBC,IAAI,GAAG,MAAM;QACf;QAEAlE,WAAW,CAACkE,IAAI,CAAC;MACnB;IACF;IAEA,OAAO,MAAM;MACXC,aAAa,CAACpB,UAAU,CAAC;MACzB,IAAIlC,MAAM,EAAE;QACVA,MAAM,CAACuD,IAAI,CAAC,CAAC;MACf;IACF,CAAC;EACH,CAAC,EAAE,CAACpE,WAAW,EAAEC,UAAU,CAAC,CAAC;EAE7B,oBACEL,OAAA;IAAAyE,QAAA,eACEzE,OAAA;MAAO0E,GAAG,EAAEpE,QAAS;MAACqE,QAAQ;MAACC,KAAK;MAACrD,KAAK,EAAC,KAAK;MAACC,MAAM,EAAC;IAAK;MAAAqD,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAE;EAAC;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OAC7D,CAAC;AAEV,CAAC;AAAC7E,EAAA,CAhGIF,2BAA2B;AAAAgF,EAAA,GAA3BhF,2BAA2B;AAkGjC,eAAeA,2BAA2B;AAAC,IAAAgF,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}